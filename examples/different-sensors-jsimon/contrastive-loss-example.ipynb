{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Contrastive Pre-Training\n",
    "We use contrative pre-training as the basis of later supervised training. The idea is to project the basic features\n",
    "first to a common space where they are invariant to some sensor specifics, and then use supervised training on top of it."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "---\n",
    "some imports"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch_utils as tu\n",
    "import utils as ut\n",
    "import models\n",
    "import simclr\n",
    "import pickle"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "---\n",
    "## Prepare the dataset\n",
    "\n",
    "We simulate four sensors with different noise and sampling rate"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "# set the important parameters for dataset generation\n",
    "batch_size = 512\n",
    "timesteps_per_example_in_100Hz = 200\n",
    "num_examples = batch_size * 20\n",
    "jiggle_offsets = 20\n",
    "\n",
    "sensor1 = ut.Sensor(40, 0.0, 0.2)\n",
    "sensor2 = ut.Sensor(80, 0.1, 0.1)\n",
    "sensor3 = ut.Sensor(80, 0.0, 0.3)\n",
    "sensor4 = ut.Sensor(20, 0.0, 0.1)\n",
    "sensors = [sensor1, sensor2, sensor3, sensor4]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "---\n",
    "They key to contrastive learning is to use augmentations for the data, we add three options + one we have directly\n",
    "in the dataset"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "transform_options = [\n",
    "    transforms.Compose([tu.AblateBlock(5,30), tu.ToTensor()]),\n",
    "    transforms.Compose([tu.AddNoise((-0.1, 0.1), (0.0, 0.3)), tu.ToTensor()]),\n",
    "    transforms.Compose([tu.RandomDownsample(), tu.ToTensor()])\n",
    "]\n",
    "trsfm = transforms.RandomChoice(transform_options)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "---\n",
    "## Generate the Dataset"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data with only two columns is assumed to have no missing value field, just return the whole index\n",
      "data with only two columns is assumed to have no missing value field, just return the whole index\n",
      "data with only two columns is assumed to have no missing value field, just return the whole index\n"
     ]
    }
   ],
   "source": [
    "dataset = tu.BadSensorsDataset(sensors,timesteps_per_example_in_100Hz, num_examples, jiggle_offsets=jiggle_offsets, transform=trsfm, return_two_transforms=True)\n",
    "\n",
    "assert len(dataset) == num_examples * len(sensors)\n",
    "assert len(dataset[0][0]) == timesteps_per_example_in_100Hz"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "This generation of the Dataset might take a bit of time, so if generated once just load it from a pickle"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [],
   "source": [
    "with open('data/dataset.pkl', 'wb') as file:\n",
    "    pickle.dump(dataset, file)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'BadSensorsDataset' object has no attribute 'labels'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-4-7cc17c549093>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      3\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      4\u001B[0m \u001B[0;32massert\u001B[0m \u001B[0mlen\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdataset\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m==\u001B[0m \u001B[0mnum_examples\u001B[0m \u001B[0;34m*\u001B[0m \u001B[0mlen\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0msensors\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 5\u001B[0;31m \u001B[0;32massert\u001B[0m \u001B[0mlen\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdataset\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m0\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m==\u001B[0m \u001B[0;36m2\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      6\u001B[0m \u001B[0;32massert\u001B[0m \u001B[0mlen\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdataset\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m0\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m0\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m==\u001B[0m \u001B[0mtimesteps_per_example_in_100Hz\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      7\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/Documents/phd/unsupervised-pretraining/torch_utils.py\u001B[0m in \u001B[0;36m__getitem__\u001B[0;34m(self, idx)\u001B[0m\n\u001B[1;32m     86\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     87\u001B[0m         \u001B[0;31m# else we assume it is a single index so:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 88\u001B[0;31m         \u001B[0msample\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mlabel\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mget_single_item\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0midx\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     89\u001B[0m         \u001B[0msample\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mapply_transforms\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0msample\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     90\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/Documents/phd/unsupervised-pretraining/torch_utils.py\u001B[0m in \u001B[0;36mget_single_item\u001B[0;34m(self, idx)\u001B[0m\n\u001B[1;32m     52\u001B[0m         \u001B[0msample_idx\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0midx\u001B[0m \u001B[0;34m%\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mnum_samples_per_sensor\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     53\u001B[0m         \u001B[0mtime_series\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msignal\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0msensor_idx\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 54\u001B[0;31m         \u001B[0mlabels\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mlabels\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0msensor_idx\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     55\u001B[0m         \u001B[0mstart\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msample_steps\u001B[0m \u001B[0;34m*\u001B[0m \u001B[0msample_idx\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     56\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mjiggle_offsets\u001B[0m \u001B[0;32mis\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mAttributeError\u001B[0m: 'BadSensorsDataset' object has no attribute 'labels'"
     ]
    }
   ],
   "source": [
    "with open('data/dataset.pkl', 'rb') as file:\n",
    "    dataset = pickle.load(file)\n",
    "\n",
    "assert len(dataset) == num_examples * len(sensors)\n",
    "assert len(dataset[0]) == 2\n",
    "assert len(dataset[0][0]) == timesteps_per_example_in_100Hz"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "---\n",
    "Train, validation and test split"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "valid_size = 0.2\n",
    "test_size = 0.1\n",
    "num_workers = 2\n",
    "\n",
    "num_train = len(dataset)\n",
    "indices = list(range(num_train))\n",
    "train_idx, valid_idx, test_idx = ut.random_splits(indices, test_size, valid_size)\n",
    "\n",
    "train_sampler = SubsetRandomSampler(train_idx)\n",
    "valid_sampler = SubsetRandomSampler(valid_idx)\n",
    "test_sampeler = SubsetRandomSampler(test_idx)\n",
    "\n",
    "# prepare data loaders (combine dataset and sampler)\n",
    "train_loader = DataLoader(dataset, batch_size=batch_size,\n",
    "    sampler=train_sampler, num_workers=num_workers)\n",
    "valid_loader = DataLoader(dataset, batch_size=batch_size,\n",
    "    sampler=valid_sampler, num_workers=num_workers)\n",
    "test_loader = DataLoader(dataset, batch_size=batch_size,\n",
    "    sampler=test_sampeler, num_workers=num_workers)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "---\n",
    "Check if a GPU is available"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is not available.  Training on CPU ...\n"
     ]
    }
   ],
   "source": [
    "train_on_gpu = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "if not train_on_gpu:\n",
    "    print('CUDA is not available.  Training on CPU ...')\n",
    "else:\n",
    "    print('CUDA is available!  Training on GPU ...')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "---\n",
    "## Train Loop\n",
    "\n",
    "Define training loop"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "def train(n_epochs, model, projection, optimizer, criterion, train_loader, valid_loader):\n",
    "    def cd(x): #cd = correct dimension\n",
    "        x = x.float()\n",
    "        x = x[:,:,1:].contiguous()\n",
    "        #print(x.shape)\n",
    "        #x = x.unsqueeze(dim=2)\n",
    "        batch_size, sequence_length, input_dim = x.shape\n",
    "        x = x.view(batch_size, input_dim, sequence_length)\n",
    "        return x\n",
    "    valid_loss_min = np.Inf # track change in validation loss\n",
    "\n",
    "    for epoch in range(1, n_epochs+1):\n",
    "        # keep track of training and validation loss\n",
    "        train_loss = 0.0\n",
    "        valid_loss = 0.0\n",
    "\n",
    "        ###################\n",
    "        # train the model #\n",
    "        ###################\n",
    "        model.train()\n",
    "        for i_batch, (sk1, sk2) in enumerate(train_loader):\n",
    "            if sk1.shape[0] != 512:\n",
    "                continue # if not full batch, just continue\n",
    "            sk1, sk2 = cd(sk1), cd(sk2)\n",
    "            #print('data', sk1.shape, sk2.shape)\n",
    "            #print('val example', sk1[5,0,:10])\n",
    "            # move tensors to GPU if CUDA is available\n",
    "            if train_on_gpu:\n",
    "                sk1, sk2 = sk1.cuda(), sk2.cuda()\n",
    "            # clear the gradients of all optimized variables\n",
    "            optimizer.zero_grad()\n",
    "            # forward pass: compute feature embeddings from model,\n",
    "            # and add non-linear projection for loss\n",
    "            h1, h2 = model(sk1), model(sk2)\n",
    "            #print('model', h1.shape, h2.shape)\n",
    "            #print('mod-val example', h1[5,:])\n",
    "            z1, z2 = projection(h1), projection(h2)\n",
    "            #print('projection', z1.shape, z2.shape)\n",
    "            #print('pr-val example', z1[5,:])\n",
    "            # calculate the batch loss\n",
    "            loss = criterion(z1, z2)\n",
    "            #print('loss: ', loss)\n",
    "            # backward pass: compute gradient of the loss with respect to model parameters\n",
    "            loss.backward()\n",
    "            # perform a single optimization step (parameter update)\n",
    "            optimizer.step()\n",
    "            # update training loss\n",
    "            train_loss += loss.item()*sk1.size(0)\n",
    "\n",
    "        ######################\n",
    "        # validate the model #\n",
    "        ######################\n",
    "        model.eval()\n",
    "        for i_batch, (sk1, sk2) in enumerate(valid_loader):\n",
    "            if sk1.shape[0] != 512:\n",
    "                continue # if not full batch, just continue\n",
    "            sk1, sk2 = cd(sk1), cd(sk2)\n",
    "            # move tensors to GPU if CUDA is available\n",
    "            if train_on_gpu:\n",
    "                sk1, sk2 = sk1.cuda(), sk2.cuda()\n",
    "            # forward pass: compute feature embeddings from model,\n",
    "            # and add non-linear projection for loss\n",
    "            h1, h2 = model(sk1), model(sk2)\n",
    "            z1, z2 = projection(h1), projection(h2)\n",
    "            # calculate the batch loss\n",
    "            loss = criterion(z1, z2)\n",
    "            # update average validation loss\n",
    "            valid_loss += loss.item()*sk1.size(0)\n",
    "\n",
    "        # calculate average losses\n",
    "        train_loss = train_loss/len(train_loader.sampler)\n",
    "        valid_loss = valid_loss/len(valid_loader.sampler)\n",
    "\n",
    "        # print training/validation statistics\n",
    "        print('Epoch: {} \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f}'.format(\n",
    "            epoch, train_loss, valid_loss))\n",
    "\n",
    "        # save model if validation loss has decreased\n",
    "        if valid_loss <= valid_loss_min:\n",
    "            print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(\n",
    "            valid_loss_min,\n",
    "            valid_loss))\n",
    "            torch.save(model.state_dict(), 'data/model_embedding.pt')\n",
    "            torch.save(projection.state_dict(), 'data/model_projection.pt')\n",
    "            valid_loss_min = valid_loss"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "model = models.InceptionModel(num_blocks=3, in_channels=2, out_channels=16,\n",
    "                           bottleneck_channels=2, kernel_sizes=41, use_residuals=True,\n",
    "                           num_pred_classes=128).float()\n",
    "\n",
    "projection = models.Projection([128,56,32,2]).float()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "model = models.SimpleEncoder()\n",
    "\n",
    "projection = models.Projection([44,22,10,2]).float()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "data": {
      "text/plain": "SimpleEncoder(\n  (encoding): Conv1d(2, 2, kernel_size=(10,), stride=(2,))\n  (layers): Sequential(\n    (0): Conv1d(2, 2, kernel_size=(10,), stride=(2,))\n    (1): ReLU()\n    (2): Conv1d(2, 1, kernel_size=(10,), stride=(2,))\n    (3): ReLU()\n  )\n)"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "data": {
      "text/plain": "Projection(\n  (layers): Sequential(\n    (0): Linear(in_features=44, out_features=22, bias=False)\n    (1): ReLU()\n    (2): Linear(in_features=22, out_features=10, bias=False)\n    (3): ReLU()\n    (4): Linear(in_features=10, out_features=2, bias=False)\n  )\n)"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "projection"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training with temp: 0.43905823466525024 and lr: 9.523780331283934e-05\n",
      "Epoch: 1 \tTraining Loss: 6.519601 \tValidation Loss: 6.285295\n",
      "Validation loss decreased (inf --> 6.285295).  Saving model ...\n",
      "Epoch: 2 \tTraining Loss: 6.305074 \tValidation Loss: 6.102453\n",
      "Validation loss decreased (6.285295 --> 6.102453).  Saving model ...\n",
      "Epoch: 3 \tTraining Loss: 6.137534 \tValidation Loss: 5.965303\n",
      "Validation loss decreased (6.102453 --> 5.965303).  Saving model ...\n",
      "training with temp: 0.8093627637294365 and lr: 1.0948920358169622e-05\n",
      "Epoch: 1 \tTraining Loss: 6.577522 \tValidation Loss: 6.391799\n",
      "Validation loss decreased (inf --> 6.391799).  Saving model ...\n",
      "Epoch: 2 \tTraining Loss: 6.502877 \tValidation Loss: 6.378729\n",
      "Validation loss decreased (6.391799 --> 6.378729).  Saving model ...\n",
      "Epoch: 3 \tTraining Loss: 6.484091 \tValidation Loss: 6.371346\n",
      "Validation loss decreased (6.378729 --> 6.371346).  Saving model ...\n",
      "training with temp: 0.667841233652229 and lr: 1.4890628566334744e-05\n",
      "Epoch: 1 \tTraining Loss: 6.755001 \tValidation Loss: 6.631856\n",
      "Validation loss decreased (inf --> 6.631856).  Saving model ...\n",
      "Epoch: 2 \tTraining Loss: 6.741659 \tValidation Loss: 6.612502\n",
      "Validation loss decreased (6.631856 --> 6.612502).  Saving model ...\n",
      "Epoch: 3 \tTraining Loss: 6.712588 \tValidation Loss: 6.588070\n",
      "Validation loss decreased (6.612502 --> 6.588070).  Saving model ...\n",
      "training with temp: 0.7037848648555682 and lr: 6.500318565461027e-05\n",
      "Epoch: 1 \tTraining Loss: 6.856902 \tValidation Loss: 6.737544\n",
      "Validation loss decreased (inf --> 6.737544).  Saving model ...\n",
      "Epoch: 2 \tTraining Loss: 6.856760 \tValidation Loss: 6.735758\n",
      "Validation loss decreased (6.737544 --> 6.735758).  Saving model ...\n",
      "Epoch: 3 \tTraining Loss: 6.856016 \tValidation Loss: 6.736393\n",
      "training with temp: 0.07173407219153158 and lr: 4.034538594444495e-05\n",
      "Epoch: 1 \tTraining Loss: 6.640199 \tValidation Loss: 6.410143\n",
      "Validation loss decreased (inf --> 6.410143).  Saving model ...\n",
      "Epoch: 2 \tTraining Loss: 6.489894 \tValidation Loss: 6.363702\n",
      "Validation loss decreased (6.410143 --> 6.363702).  Saving model ...\n",
      "Epoch: 3 \tTraining Loss: 6.446959 \tValidation Loss: 6.332016\n",
      "Validation loss decreased (6.363702 --> 6.332016).  Saving model ...\n",
      "training with temp: 0.19028337037514165 and lr: 3.111692243632022e-05\n",
      "Epoch: 1 \tTraining Loss: 6.832711 \tValidation Loss: 6.689545\n",
      "Validation loss decreased (inf --> 6.689545).  Saving model ...\n",
      "Epoch: 2 \tTraining Loss: 6.742423 \tValidation Loss: 6.522108\n",
      "Validation loss decreased (6.689545 --> 6.522108).  Saving model ...\n",
      "Epoch: 3 \tTraining Loss: 6.518127 \tValidation Loss: 6.324404\n",
      "Validation loss decreased (6.522108 --> 6.324404).  Saving model ...\n",
      "training with temp: 0.26440945364570223 and lr: 1.3095233303243656e-05\n",
      "Epoch: 1 \tTraining Loss: 6.825211 \tValidation Loss: 6.689616\n",
      "Validation loss decreased (inf --> 6.689616).  Saving model ...\n",
      "Epoch: 2 \tTraining Loss: 6.795640 \tValidation Loss: 6.641976\n",
      "Validation loss decreased (6.689616 --> 6.641976).  Saving model ...\n",
      "Epoch: 3 \tTraining Loss: 6.721070 \tValidation Loss: 6.593649\n",
      "Validation loss decreased (6.641976 --> 6.593649).  Saving model ...\n",
      "training with temp: 0.3437844136648555 and lr: 8.56942988746807e-05\n",
      "Epoch: 1 \tTraining Loss: 6.814264 \tValidation Loss: 6.684792\n",
      "Validation loss decreased (inf --> 6.684792).  Saving model ...\n",
      "Epoch: 2 \tTraining Loss: 6.802885 \tValidation Loss: 6.671040\n",
      "Validation loss decreased (6.684792 --> 6.671040).  Saving model ...\n",
      "Epoch: 3 \tTraining Loss: 6.784332 \tValidation Loss: 6.662637\n",
      "Validation loss decreased (6.671040 --> 6.662637).  Saving model ...\n",
      "training with temp: 0.4364522752075856 and lr: 8.759281566677067e-05\n",
      "Epoch: 1 \tTraining Loss: 6.833886 \tValidation Loss: 6.691134\n",
      "Validation loss decreased (inf --> 6.691134).  Saving model ...\n",
      "Epoch: 2 \tTraining Loss: 6.780232 \tValidation Loss: 6.642043\n",
      "Validation loss decreased (6.691134 --> 6.642043).  Saving model ...\n",
      "Epoch: 3 \tTraining Loss: 6.753920 \tValidation Loss: 6.631154\n",
      "Validation loss decreased (6.642043 --> 6.631154).  Saving model ...\n",
      "training with temp: 0.49623696307414905 and lr: 6.987037422664316e-05\n",
      "Epoch: 1 \tTraining Loss: 6.521112 \tValidation Loss: 6.306055\n",
      "Validation loss decreased (inf --> 6.306055).  Saving model ...\n",
      "Epoch: 2 \tTraining Loss: 6.369677 \tValidation Loss: 6.201280\n",
      "Validation loss decreased (6.306055 --> 6.201280).  Saving model ...\n",
      "Epoch: 3 \tTraining Loss: 6.278592 \tValidation Loss: 6.125243\n",
      "Validation loss decreased (6.201280 --> 6.125243).  Saving model ...\n"
     ]
    }
   ],
   "source": [
    "trails = 10\n",
    "\n",
    "for t in range(trails):\n",
    "    temperature = np.random.uniform(0.01, 1.0)#0.1\n",
    "    lr = np.random.uniform(0.00001, 0.0001)\n",
    "    #lr=0.00001\n",
    "    model = models.SimpleEncoder()\n",
    "    projection = models.Projection([44,22,10,2]).float()\n",
    "    print(f\"training with temp: {temperature} and lr: {lr}\")\n",
    "    criterion = simclr.NT_Xent(batch_size, temperature, device)\n",
    "    optimizer = torch.optim.Adam(list(model.parameters()) + list(projection.parameters()), lr=lr)\n",
    "\n",
    "    n_epochs = 3\n",
    "\n",
    "    train(n_epochs, model, projection, optimizer, criterion, train_loader, valid_loader)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "temperature = 0.439\n",
    "lr = 9.524e-05\n",
    "\n",
    "criterion = simclr.NT_Xent(batch_size, temperature, device)\n",
    "optimizer = torch.optim.Adam(list(model.parameters()) + list(projection.parameters()), lr=lr)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tTraining Loss: 6.686040 \tValidation Loss: 6.475939\n",
      "Validation loss decreased (inf --> 6.475939).  Saving model ...\n",
      "Epoch: 2 \tTraining Loss: 6.443450 \tValidation Loss: 6.216212\n",
      "Validation loss decreased (6.475939 --> 6.216212).  Saving model ...\n",
      "Epoch: 3 \tTraining Loss: 6.202213 \tValidation Loss: 6.008098\n",
      "Validation loss decreased (6.216212 --> 6.008098).  Saving model ...\n",
      "Epoch: 4 \tTraining Loss: 6.057703 \tValidation Loss: 5.939936\n",
      "Validation loss decreased (6.008098 --> 5.939936).  Saving model ...\n",
      "Epoch: 5 \tTraining Loss: 6.003219 \tValidation Loss: 5.900479\n",
      "Validation loss decreased (5.939936 --> 5.900479).  Saving model ...\n",
      "Epoch: 6 \tTraining Loss: 5.983798 \tValidation Loss: 5.884562\n",
      "Validation loss decreased (5.900479 --> 5.884562).  Saving model ...\n",
      "Epoch: 7 \tTraining Loss: 5.963393 \tValidation Loss: 5.856537\n",
      "Validation loss decreased (5.884562 --> 5.856537).  Saving model ...\n",
      "Epoch: 8 \tTraining Loss: 5.946087 \tValidation Loss: 5.835260\n",
      "Validation loss decreased (5.856537 --> 5.835260).  Saving model ...\n",
      "Epoch: 9 \tTraining Loss: 5.941979 \tValidation Loss: 5.824582\n",
      "Validation loss decreased (5.835260 --> 5.824582).  Saving model ...\n",
      "Epoch: 10 \tTraining Loss: 5.931271 \tValidation Loss: 5.806959\n",
      "Validation loss decreased (5.824582 --> 5.806959).  Saving model ...\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 10\n",
    "\n",
    "train(n_epochs, model, projection, optimizer, criterion, train_loader, valid_loader)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}