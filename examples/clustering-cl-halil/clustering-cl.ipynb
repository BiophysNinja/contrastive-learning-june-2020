{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cos = torch.nn.CosineSimilarity(dim=-1, eps=1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def augment(x):\n",
    "    shift = np.random.randint(-5, 5)\n",
    "    l = x.shape[2]\n",
    "    if shift >= 0:\n",
    "        xn = np.zeros_like(x) + x[:,:,0:1]\n",
    "        xn[:,0,shift:] = x[:,0,:l-shift]\n",
    "    else:\n",
    "        xn = np.zeros_like(x) + x[:,:,-1:]\n",
    "        xn[:,0,:shift] = x[:,0,-shift:]\n",
    "    return xn + (np.random.random(size=(1,1,1)) - 0.5) * 0.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_2N(data):\n",
    "    d1 = []\n",
    "    d2 = []\n",
    "    for d in data:\n",
    "        d1.append(torch.from_numpy(augment(d)).float())\n",
    "        d2.append(torch.from_numpy(augment(d)).float())\n",
    "    return torch.stack(d1, 0), torch.stack(d2, 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_loss(o1, o2, debug=False):\n",
    "\n",
    "    N = o1.shape[0]\n",
    "        \n",
    "    # N x N\n",
    "    sim_all = cos(o1.unsqueeze(1), o2.unsqueeze(0))\n",
    "    exp_all = torch.exp(sim_all)\n",
    "        \n",
    "    # Get matching augmentation pairs\n",
    "    mask = torch.eye(N).cuda()\n",
    "    nom_all = (exp_all * mask).sum(dim=1) * 2\n",
    "        \n",
    "    # Get other pairs    \n",
    "    mask_inv = 1 - mask\n",
    "    exp_all = exp_all * mask_inv\n",
    "\n",
    "    sum_dist_all = exp_all.sum(dim=1) + exp_all.sum(dim=0)\n",
    "\n",
    "    loss_all = -torch.log(nom_all / (nom_all + sum_dist_all))\n",
    "    loss = loss_all.sum()\n",
    "\n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Data\n",
    "all_b = []\n",
    "all_l = []\n",
    "len_data = 50\n",
    "a = np.linspace(0, 10, num=len_data)\n",
    "\n",
    "#for type in np.random.choice(6, 10000, p=[0.2, 0.1, 0.1, 0.2, 0.2, 0.2]):\n",
    "for type in np.random.choice(6, 10000, p=[0.2, 0.1, 0.1, 0.1, 0.1, 0.4]):\n",
    "\n",
    "    offset = (np.random.random() - 0.5) * 5 + 5\n",
    "    all_l.append(type)\n",
    "\n",
    "    if type == 0:\n",
    "        b = np.exp(-(a - offset)**2)\n",
    "    elif type == 1:\n",
    "        b = -np.exp(-(a - offset)**2)\n",
    "    elif type == 2:\n",
    "        b = np.exp(-(a - offset)**2 / 10)\n",
    "    elif type == 3:\n",
    "        b = -np.exp(-(a - offset)**2 / 10)\n",
    "    elif type == 4:\n",
    "        b1 = np.exp(-(a - offset)**2 / 10)\n",
    "        b2 = -np.exp(-(a - offset)**2)\n",
    "        b = b1 + b2\n",
    "    else:\n",
    "        b1 = -np.exp(-(a - offset)**2 / 10)\n",
    "        b2 = np.exp(-(a - offset)**2)\n",
    "        b = b1 + b2\n",
    "\n",
    "    b += (np.random.random(len_data) - 0.5) * 0.05\n",
    "    all_b.append(b)\n",
    "    \n",
    "data_org = np.array(all_b, dtype=np.float32)\n",
    "data_org = np.expand_dims(data_org, 1)\n",
    "data_org = np.expand_dims(data_org, 1)\n",
    "labels = np.array(all_l)\n",
    "\n",
    "unq, unq_index = np.unique(labels, return_index=True)\n",
    "\n",
    "print(\"d_shape\", data_org.shape)\n",
    "print(\"l_shape\", labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in unq_index:\n",
    "    plt.plot(data_org[i].squeeze())\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model\n",
    "\n",
    "class encoderc(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(encoderc, self).__init__()\n",
    "        self.e1 = torch.nn.Conv2d(1, 8, kernel_size=(1,5), stride=(1,2))\n",
    "        _ = torch.nn.init.xavier_uniform_(self.e1.weight, 1.5)\n",
    "        self.e2 = torch.nn.Conv2d(8, 16, kernel_size=(1,5), stride=(1,2))\n",
    "        _ = torch.nn.init.xavier_uniform_(self.e2.weight, 1.5)\n",
    "        self.e3 = torch.nn.Conv2d(16, 32, kernel_size=(1,3), stride=(1,2))\n",
    "        _ = torch.nn.init.xavier_uniform_(self.e3.weight, 1.5)\n",
    "        self.e4 = torch.nn.Conv2d(32, 64, kernel_size=(1,3), stride=(1,2))\n",
    "        _ = torch.nn.init.xavier_uniform_(self.e4.weight, 1.5)\n",
    "        self.e5 = torch.nn.Linear(64, 32)\n",
    "        _ = torch.nn.init.xavier_uniform_(self.e5.weight, 1.5)\n",
    "        self.e6 = torch.nn.Linear(32, 2)\n",
    "        _ = torch.nn.init.xavier_uniform_(self.e6.weight, 1.5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        e = fn.relu(self.e1(x))\n",
    "        e = fn.relu(self.e2(e))\n",
    "        e = fn.relu(self.e3(e))\n",
    "        e = fn.relu(self.e4(e))\n",
    "        e = torch.flatten(e, start_dim=1)\n",
    "        e = fn.relu(self.e5(e))\n",
    "        e = self.e6(e)\n",
    "        return e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = encoderc().cuda()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "num_epochs = 256\n",
    "batch_size = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    idx = np.random.permutation(len(data_org))\n",
    "    data_org = data_org[idx]\n",
    "    labels = labels[idx]\n",
    "    \n",
    "    if epoch % 10 == 0:\n",
    "        o_all = model(torch.from_numpy(data_org).cuda())\n",
    "        oan = o_all.cpu().detach().numpy()\n",
    "\n",
    "        plt.scatter(oan[:,0], oan[:,1], c=labels, cmap=\"rainbow\", alpha=0.5)\n",
    "        plt.show()\n",
    "    \n",
    "    for idx in range(0, len(data_org), batch_size):\n",
    "        data = data_org[idx:idx+batch_size]\n",
    "        \n",
    "        # Get 2N Augmentations\n",
    "        d1, d2 = get_2N(data)\n",
    "\n",
    "        # Forward\n",
    "        o1 = model(d1.cuda())\n",
    "        o2 = model(d2.cuda())\n",
    "        loss = calculate_loss(o1, o2)\n",
    "        # Backward\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    print('epoch [{}/{}], loss:{:.4f}'\n",
    "      .format(epoch + 1, num_epochs, loss.item()))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
